{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393fffc-c902-409f-8ec4-c60e2037bfa2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "import os, matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d13022-ed46-451a-83ed-840c25bc22e8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from MyUtils import transforms, utils, engine, train as transforms, utils, engine, train\n",
    "from MyUtils.utils import collate_fn\n",
    "from MyUtils.engine import train_one_epoch, evaluate\n",
    "from MyUtils.plot_statistic import plot_stats\n",
    "from MyUtils.visulaize import visualize\n",
    "import MyUtils.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42c826-6357-430d-9dce-430dc4d5ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from pprint import pprint\n",
    "#import json\n",
    "\n",
    "#with open(os.path.join('C:/Users/User/Petr/Net_2/NEW_DATASET/annotation.json'), 'r') as file:\n",
    "#    annotations = json.load(file)\n",
    "\n",
    "#pprint(annotations[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377ebb9-ae6e-4ceb-8412-a134f1b0ce63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import shutil\n",
    "\n",
    "#path = 'C:/Users/User/Petr/Net_2/NEW_DATASET/9c_dataset/'\n",
    "\n",
    "#images = [file for file in sorted(os.listdir(os.path.join(path, \"images\")))]\n",
    "#annotations = [file for file in sorted(os.listdir(os.path.join(path, \"annotations\"))) if not file=='classes.txt']\n",
    "#count = 0\n",
    "#for annotation in annotations:\n",
    "#    with open(path + 'annotations/' + annotation, 'r') as file:\n",
    "#        lines = file.readlines()\n",
    "#        num_corners = len([line for line in lines if line.startswith('0')])\n",
    "#        if 2 <= num_corners <= 5:\n",
    "#            shutil.copyfile(path + 'annotations/' + annotation, f'C:/Users/User/Petr/Net_2/NEW_DATASET/5c_dataset/annotations/{annotation}')\n",
    "#            shutil.copyfile(path + 'images/' + annotation[:-4] + '.jpg', f'C:/Users/User/Petr/Net_2/NEW_DATASET/5c_dataset/images/{annotation[:-4]}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c5de8-d36d-4727-8497-5bc114ad9180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#temp_folder = './test_dataset/train/temp/'\n",
    "\n",
    "#imgs = os.listdir(temp_folder)\n",
    "\n",
    "#for num, img in enumerate(imgs):\n",
    "#    os.rename(os.path.join(temp_folder, img), os.path.join(temp_folder, f'dataset_{num + 1}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92081ec6-b18d-4fbe-a3d0-80e1f624abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=20, always_apply=False, p=0.5),\n",
    "            A.PixelDropout(dropout_prob=0.005, per_channel=False, drop_value=0, mask_drop_value=None, always_apply=False, p=0.5),\n",
    "            A.Affine(scale=(0.8, 1.0), translate_percent=(-0.1, 0.1), translate_px=None, rotate=0, shear=None, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode=0, fit_output=False, keep_ratio=True, rotate_method='largest_box', always_apply=False, p=0.5),\n",
    "            #A.RandomScale(scale_limit=[0.1, 0.5], interpolation=1, always_apply=False, p=0.5),\n",
    "            #A.CropAndPad(px=None, percent=-0.2, pad_mode='BORDER_CONSTANT', pad_cval=100, pad_cval_mask=0, keep_size=False, sample_independently=True, interpolation=1, always_apply=False, p=1.0),\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=0.1, brightness_by_max=True, always_apply=False, p=0.5), # Random change of brightness & contrast\n",
    "            #A.ToGray(p=1),\n",
    "            #A.InvertImg(p=0.5),\n",
    "            #A.ToGray(p=1)\n",
    "            #A.Sharpen(alpha=(0.5, 0.5), p=1)\n",
    "        ])\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )\n",
    "\n",
    "def eval_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            #A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            #A.RandomBrightnessContrast(brightness_limit=(-0.5, 0.0), contrast_limit=0.0, brightness_by_max=True, always_apply=False, p=1.0), # Random change of brightness & contrast\n",
    "            #A.InvertImg(p=1),\n",
    "            #A.ToGray(p=1)\n",
    "            #A.Sharpen(alpha=(0.5, 0.5), p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )\n",
    "\n",
    "def test_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            #A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            #A.RandomBrightnessContrast(brightness_limit=(-0.5, 0.0), contrast_limit=0.0, brightness_by_max=True, always_apply=False, p=1.0), # Random change of brightness & contrast\n",
    "            #A.InvertImg(p=1),\n",
    "            #A.ToGray(p=1)\n",
    "            #A.Sharpen(alpha=(0.1, 0.9), p=1)\n",
    "        ], p=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faccd13-1c73-40a2-9725-4410dbcb8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/User/Petr/Net_2/NEW_DATASET'\n",
    "\n",
    "KEYPOINTS_FOLDER = path\n",
    "KEYPOINTS_FOLDER_TRAIN = path\n",
    "\n",
    "dataset = MyUtils.Dataset.NCornerDataset(KEYPOINTS_FOLDER, transform=None, demo=False, corners=[2, 3])\n",
    "\n",
    "dataset.explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487b878-f12f-471f-bd7b-93826db812ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "idx = torch.IntTensor.item(batch[1][0]['image_id'])\n",
    "\n",
    "print(dataset.get_image(dataset.annotations[idx]))\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp for kp in kps])\n",
    "\n",
    "print(keypoints)\n",
    "visualize(image, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1b946-0508-4f93-b3da-6bfb93f3cc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MyUtils.Dataset.NCornerDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True, corners=[2, 3])\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "idx = torch.IntTensor.item(batch[1][0]['image_id'])\n",
    "\n",
    "print(dataset.get_image(dataset.annotations[idx]))\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)\n",
    "print(bboxes)\n",
    "print(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9222a7-8380-48ec-89b4-71c1a6a16e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(num_keypoints, weights_path=None):\n",
    "   \n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0, 4.0))\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=num_keypoints,\n",
    "                                                                   num_classes = 2, # Background is the first class, object is the second class\n",
    "                                                                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_V2(num_keypoints, weights_path=None):\n",
    "    from torchvision.ops import MultiScaleRoIAlign\n",
    "    from torchvision.models.detection import KeypointRCNN\n",
    "    \n",
    "    backbone = torchvision.models.mobilenet_v2(weights='DEFAULT').features\n",
    "    \n",
    "    backbone.out_channels = 1280\n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512, 1024),), aspect_ratios=((0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0, 4.0),))\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=7, sampling_ratio=2)\n",
    "    \n",
    "    \n",
    "    keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],  output_size=14, sampling_ratio=2)\n",
    "    \n",
    "    model = KeypointRCNN(backbone, num_classes=2, num_keypoints=num_keypoints, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler, keypoint_roi_pool=keypoint_roi_pooler)#, min_size=400, max_size=2000\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model\n",
    "\n",
    "def get_model_V3(num_keypoints, weights_path=None):\n",
    "    from torchvision.models.detection.keypoint_rcnn import KeypointRCNNPredictor\n",
    "    from torchvision.models.detection.rpn import AnchorGenerator\n",
    "    from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
    "    # Load a pre-trained model\n",
    "    model = keypointrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "# Replace the classifier head with the number of keypoints\n",
    "    in_features = model.roi_heads.keypoint_predictor.kps_score_lowres.in_channels\n",
    "    model.roi_heads.keypoint_predictor = KeypointRCNNPredictor(in_channels=in_features, num_keypoints=num_keypoints)\n",
    "\n",
    "# Set the model's device and data type\n",
    "    model.name = 'keypointrcnn_resnet50_fpn'\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c7572-0aeb-4870-88f1-979e3cf25304",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = 'C:/Users/User/Petr/Net_2/save_model'\n",
    "log_path = 'C:/Users/User/Petr/Net_2/Metric_log'\n",
    "\n",
    "for num in range(1):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device = torch.device('cpu')\n",
    "    \n",
    "    KEYPOINTS_FOLDER_TRAIN = path\n",
    "    KEYPOINTS_FOLDER_EVAL = path\n",
    "    corners = [2,3]\n",
    "    POINTS = max(corners)\n",
    "\n",
    "    dataset_train = MyUtils.Dataset.NCornerDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False, corners=corners)\n",
    "    dataset_test = MyUtils.Dataset.NCornerDataset(KEYPOINTS_FOLDER_EVAL, transform=eval_transform(), demo=False, corners=corners)\n",
    "\n",
    "    indices = torch.randperm(len(dataset_train)).tolist()\n",
    "    thirty_pc = int(len(dataset_train) * 0.15)\n",
    "    dataset_train = torch.utils.data.Subset(dataset_train, indices[:-thirty_pc])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-thirty_pc:])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = get_model_V3(num_keypoints = POINTS)#, weights_path='./save_model/weights_rebuild_model_2.pth'\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    #for p in model.parameters():\n",
    "    #    p.requires_grad = False\n",
    "    #for p in model.roi_heads.keypoint_head.parameters():\n",
    "    #    p.requires_grad = True\n",
    "    #for p in model.roi_heads.keypoint_predictor.parameters():\n",
    "    #    p.requires_grad = True\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    #params1 = [p for p in model.roi_heads.keypoint_head.parameters() if p.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.SGD(params, lr=2e-4, momentum=0.90)#, weight_decay=0.0001\n",
    "    \n",
    "    #optimizer = torch.optim.SGD([{'params': params1},\n",
    "    #                             {'params': model.roi_heads.keypoint_predictor.parameters(), 'lr': .001},], lr=0.001, momentum=0.90)#, weight_decay=0.0001\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "    num_epochs = 1001\n",
    "\n",
    "    kps_stats = []\n",
    "    bbox_stats = []\n",
    "\n",
    "    loss_bb = []\n",
    "    loss_kp = []\n",
    "    loss = []\n",
    "\n",
    "    e_done = 0\n",
    "\n",
    "    for epoch in range(e_done, num_epochs):\n",
    "        logger = train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=20)\n",
    "        lr_scheduler.step()\n",
    "        evaluator = evaluate(model, data_loader_test, device)\n",
    "        \n",
    "        kps_stats.append(evaluator.coco_eval['keypoints'].stats[:6])\n",
    "        bbox_stats.append(evaluator.coco_eval['bbox'].stats[:6])\n",
    "    \n",
    "        loss_bb.append(logger.meters['loss_box_reg'].global_avg)\n",
    "        loss_kp.append(logger.meters['loss_keypoint'].global_avg)\n",
    "        loss.append(logger.meters['loss'].global_avg)\n",
    "        \n",
    "        if epoch % 100 == 0 and epoch > 0:\n",
    "            torch.save(model.state_dict(), f'{save_path}/LS/2_3c/weights_{epoch}.pth')\n",
    "            plot_stats(epoch+1 - e_done, bbox_stats, kps_stats, loss_bb, loss_kp, loss, num=epoch, log_path=log_path)#, num=num\n",
    "            \n",
    "    plot_stats(num_epochs - e_done, bbox_stats, kps_stats, loss_bb, loss_kp, loss, num=(epoch+1), show=True, log_path=log_path)\n",
    "#print(type(meters[0]['loss_keypoint']))\n",
    "\n",
    "# Save model weights after training\n",
    "#torch.save(model.state_dict(), './save_model/weights_2.pth')\n",
    "    #torch.save(model.state_dict(), f'{save_path}/2c/weights_2c_re_1000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0405cb5-1c10-4db3-be9a-9e9c93f2fc59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KEYPOINTS_FOLDER = './'\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_test = get_model_V2(num_keypoints = 2, weights_path='./save_model/weights_2c_2200.pth')\n",
    "model_test.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489340b-6c00-465a-bfd2-5fed606156d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_path = 'C:/Users/User/Petr/Net_2/check_model/Rod/images/'\n",
    "\n",
    "for image in tqdm(os.listdir(test_path)):\n",
    "    img = cv2.imdecode(np.fromfile(os.path.join(test_path, image), dtype=np.uint8), cv2.IMREAD_UNCHANGED) #cv2.imread(os.path.join(test_path, image))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img_orig = cv2.imdecode(np.fromfile(os.path.join(test_path, image), dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    img_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = F.to_tensor(img)\n",
    "    type(img)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        model_test.to(device)\n",
    "        model_test.eval()\n",
    "        out = model_test([img,])\n",
    "        out = out[0]\n",
    "    #img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "    scores = out['scores'].detach().cpu().numpy()\n",
    "    kps_scores = out['keypoints_scores'].detach().cpu().numpy()\n",
    "\n",
    "    high_scores_idxs = np.where(scores > 0.75)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "    post_nms_idxs = torchvision.ops.nms(out['boxes'][high_scores_idxs], out['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "#    print(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0])\n",
    "    if not len(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "        good_kps_idxs = [[]]\n",
    "    else:\n",
    "        good_kps_idxs = np.where(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0] > 5.0) # for i in range(len(post_nms_idxs))\n",
    "#    print(good_kps_idxs[0])\n",
    "#    print('KPs: ', out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0][good_kps_idxs[0]])\n",
    "    #kps_scores_filtered = out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0][good_kps_idxs[0]]\n",
    "#print(kps_scores_filtered)\n",
    "\n",
    "#kps_filtered =  output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "#out['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "#print(kps_filtered)\n",
    "\n",
    "    keypoints = []\n",
    "    for kps in out['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "        keypoints.append([list(map(int, kp)) for kp in kps[good_kps_idxs[0]]])\n",
    "    scores = kps_scores[0][good_kps_idxs[0]]\n",
    "    #kps_scores = out['keypoints_scores'][high_scores_idxs][post_nms_idxs][good_kps_idxs[0]].detach().cpu().numpy()[0]\n",
    "#kps_idxs = np.where(output[0]['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0] > 1.2)[0]\n",
    "#print(kps_idxs)\n",
    "#print(np.array(keypoints)[0][kps_idxs])\n",
    "#keypoints = [np.array(keypoints)[0][kps_idxs].tolist()]\n",
    "    #print(keypoints)\n",
    "#keypoints = average_close_points(keypoints)\n",
    "    #print(keypoints)\n",
    "    #print(kps_scores[0][good_kps_idxs[0]])\n",
    "    #print(high_scores_idxs, post_nms_idxs, good_kps_idxs[0])\n",
    "    #break\n",
    "    bboxes = []\n",
    "    for bbox in out['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "        bboxes.append(list(map(int, bbox.tolist())))\n",
    "    bld_scores = out['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy().tolist()\n",
    "    #print(bld_scores)\n",
    "    #print(len(bboxes), len(bld_scores))\n",
    "    #print(type(out[0]['boxes'].detach().cpu().numpy().tolist()))\n",
    "    visualize(img_orig, bboxes, keypoints, label=image, bld_score=bld_scores, kps_scores=scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
