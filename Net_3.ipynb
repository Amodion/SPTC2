{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393fffc-c902-409f-8ec4-c60e2037bfa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "import os, matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d13022-ed46-451a-83ed-840c25bc22e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from MyUtils import transforms, utils, engine, train as transforms, utils, engine, train\n",
    "from MyUtils.utils import collate_fn\n",
    "from MyUtils.engine import train_one_epoch, evaluate\n",
    "from MyUtils.plot_statistic import plot_stats\n",
    "from MyUtils.visualize import visualize\n",
    "import MyUtils.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92081ec6-b18d-4fbe-a3d0-80e1f624abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.OneOf([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=0.5), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True, always_apply=False, p=0.5), # Random change of brightness & contrast\n",
    "            #A.ToGray(p=1),\n",
    "            #A.InvertImg(p=1),\n",
    "            #A.ToGray(p=1)\n",
    "            #A.Sharpen(alpha=(0.5, 0.5), p=1),\n",
    "            A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, always_apply=False, p=0.5)\n",
    "            ]),\n",
    "        A.Sequential([\n",
    "            A.RandomScale(scale_limit=0.1, interpolation=1, always_apply=False, p=0.5)\n",
    "        ]),\n",
    "    ], p=1.0)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )\n",
    "\n",
    "def eval_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            #A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            #A.RandomBrightnessContrast(brightness_limit=(-0.5, 0.5), contrast_limit=(-0.2, 0.2), brightness_by_max=True, always_apply=False, p=1.0), # Random change of brightness & contrast\n",
    "            #A.InvertImg(p=1),\n",
    "            #A.ToGray(p=1)\n",
    "            #A.Sharpen(alpha=(0.5, 0.5), p=1)\n",
    "        ], p=1)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    )\n",
    "\n",
    "def test_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            #A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.5, 0.0), contrast_limit=0.0, brightness_by_max=True, always_apply=False, p=1.0), # Random change of brightness & contrast\n",
    "            #A.InvertImg(p=1),\n",
    "            #A.ToGray(p=1)\n",
    "            #A.Sharpen(alpha=(0.1, 0.9), p=1)\n",
    "        ], p=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03babd-7143-466d-b00d-90093f1a6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/User/Petr/Net_3/Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9e3e5-7f95-447e-88ac-b1a6fcfaea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyUtils.Dataset.PillarsDataset(path, transform=None, demo=False)\n",
    "\n",
    "dataset.explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487b878-f12f-471f-bd7b-93826db812ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "idx = torch.IntTensor.item(batch[1][0]['image_id'])\n",
    "\n",
    "print(f'ID: {dataset.inner_id}')\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp for kp in kps])\n",
    "\n",
    "print(bboxes)\n",
    "visualize(image, bboxes, keypoints, save_path=os.path.join(path, 'check_dataset', '1.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa37ab-5953-4252-9730-f0abab3143c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyUtils.Dataset.PillarsDataset(path, transform=train_transform(), demo=True)\n",
    "dataset.explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1b946-0508-4f93-b3da-6bfb93f3cc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "iterator = iter(data_loader)\n",
    "batch = next(iterator)\n",
    "\n",
    "image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints = []\n",
    "for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints.append([kp for kp in kps])\n",
    "\n",
    "image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n",
    "\n",
    "keypoints_original = []\n",
    "for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n",
    "    keypoints_original.append([kp for kp in kps])\n",
    "\n",
    "visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original, save_path=os.path.join(path, 'check_dataset', '1.jpg'))\n",
    "print(f'ID: {dataset.inner_id}')\n",
    "print(bboxes)\n",
    "print(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9222a7-8380-48ec-89b4-71c1a6a16e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pillar_detection_model(num_classes, pretrain=True, weights_path=None):\n",
    "    from torch import nn\n",
    "    import torchvision\n",
    "    from torchvision.models.detection import FasterRCNN\n",
    "    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "    from torchvision.models.detection.rpn import AnchorGenerator\n",
    "    # load a model pre-trained pre-trained on COCO\n",
    "    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0, 4.0))\n",
    "    \n",
    "    if pretrain:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT', min_size=1500, max_size=5000, class_weigth=[1.0, 3.0])\n",
    "    else:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1) # lenght of classes WITHOUT background on input\n",
    "    \n",
    "    #model.rpn.anchor_generator = anchor_generator\n",
    "    \n",
    "#    model.roi_heads.box_predictor.cls_score = nn.Sequential(nn.Linear(1024, 512), nn.Linear(512,256), nn.Linear(256, 128), nn.Linear(128, 2))\n",
    "    \n",
    "#    model.roi_heads.box_predictor.bbox_pred = nn.Sequential(nn.Linear(1024, 512), nn.Linear(512,256), nn.Linear(256, 128), nn.Linear(128, 8))\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def get_model_V3(num_keypoints, weights_path=None):\n",
    "    from torchvision.models.detection.keypoint_rcnn import KeypointRCNNPredictor\n",
    "    from torchvision.models.detection.rpn import AnchorGenerator\n",
    "    from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
    "    # Load a pre-trained model\n",
    "    model = keypointrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "# Replace the classifier head with the number of keypoints\n",
    "    in_features = model.roi_heads.keypoint_predictor.kps_score_lowres.in_channels\n",
    "    model.roi_heads.keypoint_predictor = KeypointRCNNPredictor(in_channels=in_features, num_keypoints=num_keypoints)\n",
    "\n",
    "# Set the model's device and data type\n",
    "    model.name = 'keypointrcnn_resnet50_fpn'\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ae891-b7f7-40d9-b6b9-05c6be596481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = get_pillar_detection_model(num_classes=5)\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6d581-8cc7-49ce-9409-02b3a0f4e9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_path = 'C:/Users/User/Petr/Net_1/Source/save_model/model_weights_fourth_Alb_0.38.pth'\n",
    "#classes = {'Building': 1}\n",
    "#path = 'C:/Users/User/Petr/temp_images'\n",
    "\n",
    "#model_d = load_model(model_path)\n",
    "\n",
    "#backbone = torchvision.models.mobilenet_v2(weights='DEFAULT').features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4363fb-b193-407f-9c6a-6462eb38e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyUtils.Dataset.PillarsDataset(path, transform=train_transform(), demo=False)\n",
    "dataset_test = MyUtils.Dataset.PillarsDataset(path, transform=eval_transform(), demo=False)\n",
    "\n",
    "dataset_train.explore\n",
    "dataset_test.explore\n",
    "\n",
    "points = dataset_train.get_max_corners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c7572-0aeb-4870-88f1-979e3cf25304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = 'C:/Users/User/Petr/Net_3/save_model'\n",
    "log_path = 'C:/Users/User/Petr/Net_3/Metric_log'\n",
    "\n",
    "for num in range(1):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    indices = torch.randperm(len(dataset_train)).tolist()\n",
    "    thirty_pc = int(len(dataset_train) * 0.20)\n",
    "    dataset_train = torch.utils.data.Subset(dataset_train, indices[:-thirty_pc])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-thirty_pc:])\n",
    "\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = get_model_V3(num_keypoints=points, weights_path=f'{save_path}/LS/weights_220.pth')#, weights_path=f'{save_path}/LS/weights_70.pth'\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    #params1 = [p for p in model.roi_heads.keypoint_head.parameters() if p.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.SGD(params, lr=1.8e-5, momentum=0.90)#, weight_decay=0.0001\n",
    "    \n",
    "    #optimizer = torch.optim.SGD([{'params': params1},\n",
    "    #                             {'params': model.roi_heads.keypoint_predictor.parameters(), 'lr': .001},], lr=0.001, momentum=0.90)#, weight_decay=0.0001\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
    "    num_epochs = 501\n",
    "\n",
    "    kps_stats = []\n",
    "    bbox_stats = []\n",
    "\n",
    "    loss_bb = []\n",
    "    loss_kp = []\n",
    "    loss = []\n",
    "\n",
    "    e_done = 220\n",
    "\n",
    "    for epoch in range(e_done+1, num_epochs):\n",
    "        logger = train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=20)\n",
    "        lr_scheduler.step()\n",
    "        evaluator = evaluate(model, data_loader_test, device)\n",
    "        \n",
    "        kps_stats.append(evaluator.coco_eval['keypoints'].stats[:6])\n",
    "        bbox_stats.append(evaluator.coco_eval['bbox'].stats[:6])\n",
    "    \n",
    "        loss_bb.append(logger.meters['loss_box_reg'].global_avg)\n",
    "        loss_kp.append(logger.meters['loss_keypoint'].global_avg)\n",
    "        loss.append(logger.meters['loss'].global_avg)\n",
    "        \n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            torch.save(model.state_dict(), f'{save_path}/LS/weights_{epoch}.pth')\n",
    "            plot_stats(epoch - e_done, bbox_stats, kps_stats, loss_bb, loss_kp, loss, num=epoch, log_path=log_path)#, num=num\n",
    "            \n",
    "    plot_stats(num_epochs - e_done, bbox_stats, kps_stats, loss_bb, loss_kp, loss, num=epoch, show=True, log_path=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e07a9e-7329-4518-887f-9e6436a52b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'C:/Users/User/Petr/Net_3/test_model/images/'\n",
    "out_path = 'C:/Users/User/Petr/Net_3/test_model/pred/'\n",
    "\n",
    "save_path = 'C:/Users/User/Petr/Net_3/save_model'\n",
    "#model = get_model_V3(num_keypoints=points, weights_path=f'{save_path}/LS/weights_200.pth')\n",
    "\n",
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "for image in tqdm(os.listdir(test_path)):\n",
    "    img = cv2.imdecode(np.fromfile(os.path.join(test_path, image), dtype=np.uint8), cv2.IMREAD_UNCHANGED) #cv2.imread(os.path.join(test_path, image))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img_orig = cv2.imdecode(np.fromfile(os.path.join(test_path, image), dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    img_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = F.to_tensor(img)\n",
    "    type(img)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        #model_test.to(device)\n",
    "        model.eval()\n",
    "        out = model([img,])\n",
    "        out = out[0]\n",
    "    #img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "    scores = out['scores'].detach().cpu().numpy()\n",
    "    kps_scores = out['keypoints_scores'].detach().cpu().numpy()\n",
    "\n",
    "    high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "    post_nms_idxs = torchvision.ops.nms(out['boxes'][high_scores_idxs], out['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "#    print(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0])\n",
    "    if not len(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "        good_kps_idxs = [[]]\n",
    "    else:\n",
    "        good_kps_idxs = np.where(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0] > -2.0) # for i in range(len(post_nms_idxs))\n",
    "#    print(good_kps_idxs[0])\n",
    "#    print('KPs: ', out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0][good_kps_idxs[0]])\n",
    "    #kps_scores_filtered = out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0][good_kps_idxs[0]]\n",
    "#print(kps_scores_filtered)\n",
    "\n",
    "#kps_filtered =  output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "#out['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "#print(kps_filtered)\n",
    "\n",
    "    keypoints = []\n",
    "    for kps in out['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "        keypoints.append([list(map(int, kp)) for kp in kps[good_kps_idxs[0]]])\n",
    "    scores = kps_scores[0][good_kps_idxs[0]]\n",
    "    #kps_scores = out['keypoints_scores'][high_scores_idxs][post_nms_idxs][good_kps_idxs[0]].detach().cpu().numpy()[0]\n",
    "#kps_idxs = np.where(output[0]['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0] > 1.2)[0]\n",
    "#print(kps_idxs)\n",
    "#print(np.array(keypoints)[0][kps_idxs])\n",
    "#keypoints = [np.array(keypoints)[0][kps_idxs].tolist()]\n",
    "    #print(keypoints)\n",
    "#keypoints = average_close_points(keypoints)\n",
    "    #print(keypoints)\n",
    "    #print(kps_scores[0][good_kps_idxs[0]])\n",
    "    #print(high_scores_idxs, post_nms_idxs, good_kps_idxs[0])\n",
    "    #break\n",
    "    bboxes = []\n",
    "    for bbox in out['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "        bboxes.append(list(map(int, bbox.tolist())))\n",
    "    bld_scores = out['scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy().tolist()\n",
    "    #print(bld_scores)\n",
    "    #print(len(bboxes), len(bld_scores))\n",
    "    #print(type(out[0]['boxes'].detach().cpu().numpy().tolist()))\n",
    "    visualize(img_orig, bboxes, keypoints, bld_score=bld_scores, kps_scores=scores, save_path=out_path + image, obj_type='Pillar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6124db01-af95-41be-b6ff-f9fb4e98cf76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model_V2(num_keypoints = 9, weights_path='./save_model/weights_new_dataset.pth')\n",
    "model.to(device);\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d899099-cfa4-4743-881d-44ee67028092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "path = 'C:/Users/User/Desktop/Teobox_Shipunovo_2022-10-16T21.03.14/export_shapes_2/shape_556/'\n",
    "path_E = './check_model/images'\n",
    "\n",
    "images = []\n",
    "\n",
    "\n",
    "\n",
    "for num, label in enumerate(os.listdir(path_E)):\n",
    "    #print(os.path.join(path_E, label))\n",
    "    #if num >= 15:\n",
    "    #    break\n",
    "    if 'world' in label:\n",
    "        continue\n",
    "    img = cv2.imread(os.path.join(path_E, label))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = F.to_tensor(img)\n",
    "    img = img.to(device)\n",
    "    images.append((img, label))\n",
    "\n",
    "images_t = []\n",
    "for num, label in enumerate(os.listdir(path_E)):\n",
    "    #if num >= 15:\n",
    "    #    break\n",
    "    if 'world' in label:\n",
    "        continue\n",
    "    img = cv2.imread(os.path.join(path_E, label))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #img = F.to_tensor(img)\n",
    "    #img = img.to(device)\n",
    "    images_t.append((img, label))\n",
    "#print(type(images[0][0]))\n",
    "images_transformed = []\n",
    "for pair in images_t:\n",
    "    #img = A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1)(image=pair[0])['image']\n",
    "    #img = A.InvertImg(p=1)(image=img)['image']\n",
    "    #img = A.Sharpen(alpha=(0.1, 0.9), p=1)(image=img)['image']\n",
    "    img = test_transform()(image=pair[0])['image']\n",
    "    img = F.to_tensor(img)\n",
    "    img = img.to(device)\n",
    "    images_transformed.append(img)\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    images_to_pred = images_transformed\n",
    "    output = model(images_to_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c8fb2-4cb3-4eca-9ec0-01e06b5acb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model_V2(num_keypoints = 9, weights_path='./save_model/weights_rebuild_model_3.pth')\n",
    "model.to(device);\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649a118-b656-4e96-b0bf-4ba093fbdadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'C:/Users/User/Desktop/Teobox_Shipunovo_2022-10-16T21.03.14/export_shapes/shape_401/'\n",
    "path_E = 'C:/Users/User/Petr/Net_2/test_dataset/two_dataset/eval/images'\n",
    "\n",
    "images = []\n",
    "\n",
    "\n",
    "\n",
    "for num, label in enumerate(os.listdir(path)):\n",
    "    if num >= 15:\n",
    "        break\n",
    "    if 'world' in label:\n",
    "        continue\n",
    "    img = cv2.imread(os.path.join(path, label))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = F.to_tensor(img)\n",
    "    img = img.to(device)\n",
    "    images.append((img, label))\n",
    "#print(images)\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    images_to_pred = [image for (image, label) in images]\n",
    "    output = model(images_to_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e760cf-8fbe-45fa-920c-7d15aabac3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_object_detection_model(num_classes):\n",
    "    from torch import nn\n",
    "    import torchvision\n",
    "    from torchvision.models.detection import FasterRCNN\n",
    "    from torchvision.models.detection.rpn import AnchorGenerator\n",
    "    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "    \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # lenght of classes WITHOUT background on input\n",
    "    \n",
    "#    model.roi_heads.box_predictor.cls_score = nn.Sequential(nn.Linear(1024, 512), nn.Linear(512,256), nn.Linear(256, 128), nn.Linear(128, 2))\n",
    "    \n",
    "#    model.roi_heads.box_predictor.bbox_pred = nn.Sequential(nn.Linear(1024, 512), nn.Linear(512,256), nn.Linear(256, 128), nn.Linear(128, 8))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(filepath: str):   \n",
    "    model = get_object_detection_model(num_classes=2)\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7652fc-cdb4-413f-b673-85d5efc245bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for image, out in zip(images, output):\n",
    "    label = image[1][:-4] + 'big_ds'\n",
    "    image = (image[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "    scores = out['scores'].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    high_scores_idxs = np.where(scores > 0.75)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "    post_nms_idxs = torchvision.ops.nms(out['boxes'][high_scores_idxs], out['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n",
    "\n",
    "# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n",
    "# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n",
    "# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n",
    "#    print(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0])\n",
    "    if not len(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()):\n",
    "        good_kps_idxs = [[]]\n",
    "    else:\n",
    "        good_kps_idxs = np.where(out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0] > 0.0) # for i in range(len(post_nms_idxs))\n",
    "#    print(good_kps_idxs[0])\n",
    "#    print('KPs: ', out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0][good_kps_idxs[0]])\n",
    "    #kps_scores_filtered = out['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0][good_kps_idxs[0]]\n",
    "#print(kps_scores_filtered)\n",
    "\n",
    "#kps_filtered =  output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "#out['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()\n",
    "#print(kps_filtered)\n",
    "\n",
    "    keypoints = []\n",
    "    for kps in out['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "        keypoints.append([list(map(int, kp)) for kp in kps[good_kps_idxs[0]]])\n",
    "#kps_idxs = np.where(output[0]['keypoints_scores'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy()[0] > 1.2)[0]\n",
    "#print(kps_idxs)\n",
    "#print(np.array(keypoints)[0][kps_idxs])\n",
    "#keypoints = [np.array(keypoints)[0][kps_idxs].tolist()]\n",
    "    print(keypoints)\n",
    "#keypoints = average_close_points(keypoints)\n",
    "#print(keypoints)\n",
    "    \n",
    "    bboxes = []\n",
    "    for bbox in out['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "        bboxes.append(list(map(int, bbox.tolist())))\n",
    "    \n",
    "\n",
    "    \n",
    "    visualize(image, bboxes, keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992614d-0c0e-40d6-a469-f589075ce72d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for file in two_corner_files:\n",
    "    #shutil.copy(f'./test_dataset/train/images/{file}.jpg', f'./test_dataset/two_dataset/train/images/{file}.jpg')\n",
    "    #shutil.copy(f'./test_dataset/train/annotations/{file}.txt', f'./test_dataset/two_dataset/train/annotations/{file}.txt')\n",
    "    \n",
    "    shutil.copy(f'./test_dataset/eval/images/{file}.jpg', f'./test_dataset/two_dataset/eval/images/{file}.jpg')\n",
    "    shutil.copy(f'./test_dataset/eval/annotations/{file}.txt', f'./test_dataset/two_dataset/eval/annotations/{file}.txt')\n",
    "    \n",
    "for file in three_corners_files:\n",
    "    #shutil.copy(f'./test_dataset/train/images/{file}.jpg', f'./test_dataset/three_dataset/train/images/{file}.jpg')\n",
    "    #shutil.copy(f'./test_dataset/train/annotations/{file}.txt', f'./test_dataset/three_dataset/train/annotations/{file}.txt')\n",
    "    \n",
    "    shutil.copy(f'./test_dataset/eval/images/{file}.jpg', f'./test_dataset/three_dataset/eval/images/{file}.jpg')\n",
    "    shutil.copy(f'./test_dataset/eval/annotations/{file}.txt', f'./test_dataset/three_dataset/eval/annotations/{file}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69907d09-6272-4c95-ba23-49404709be60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for file in os.listdir('./annotations/'):\n",
    "    if file[:-4] not in extra_annotations:\n",
    "        shutil.copy(f'./annotations/{file}', f'./annotations_clear/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc7337-ccff-4a80-9df1-22a491ae7f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "def average_close_points(point_coords): \n",
    "    #point_coords = np.array([point[:-1] for point in point_coords[0]])\n",
    "    point_coords = np.array(point_coords[0])\n",
    "    maping = np.abs(point_coords[:, None, None] - point_coords[None, :, None]) <= 3\n",
    "    print(maping)\n",
    "    maping = maping.reshape(maping.shape[0], maping.shape[1], 3)\n",
    "    print(maping)\n",
    "    res = []\n",
    "    for point in maping:\n",
    "        res.append(np.average(point_coords, axis=0, weights=point))\n",
    "    #point_coords = np.average(point_coords, axis=0, weights=maping)\n",
    "    \n",
    "    return [np.unique(res, axis=0).astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd60b4-6b4e-474e-bb6c-e552c6da1f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_close_points(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a2930-8734-46e2-8d7e-63783acca3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "\n",
    "model_v = get_model(num_keypoints=4)\n",
    "\n",
    "model_graph = draw_graph(model_v, input_size=(1, 3, 128, 128))\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5ea53-51e9-45d1-8f8b-fd6a0db0d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as Fnn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16,kernel_size=(2,2),stride=1,padding='valid')\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=16, out_channels=32,kernel_size=(2,2),stride=1,padding='same')\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=(2,2),stride=1,padding='valid')\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=64,out_channels=32,kernel_size=(2,2),stride=1,padding='same')\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.linear1 = nn.Linear(in_features=32*149*149,out_features=500)\n",
    "        self.linear2 = nn.Linear(in_features=500,out_features=250)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.linear3 = nn.Linear(in_features=250,out_features=18)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x = cv2.resize(x, (600, 600))\n",
    "        x = Fnn.relu(self.conv1(x))\n",
    "        x = Fnn.relu(self.conv1_1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = Fnn.relu(self.conv2(x))\n",
    "        x = Fnn.relu(self.conv2_2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1,32*149*149) \n",
    "        x = self.dropout1(x)\n",
    "        x = Fnn.relu(self.linear1(x))\n",
    "        x = Fnn.relu(self.linear2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba26a5-367f-4bb4-8f4d-395514f685ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_self = MyModel()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.001)\n",
    "#device = torch.device('cuda')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_self.to(device)\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = './NEW_DATASET'\n",
    "KEYPOINTS_FOLDER_EVAL = './NEW_DATASET'\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_EVAL, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(data_loader_train):\n",
    "         # [[[x, y, v], [x, y, v], [0 , 0, 0], [0 , 0, 0], [0 , 0, 0], [0 , 0, 0], [0 , 0, 0], [0 , 0, 0], [0 , 0, 0], [0 , 0, 0]]]\n",
    "        points = [point for point in labels[0]['keypoints'][0]]\n",
    "        kps = []\n",
    "        for point in points:\n",
    "            x, y, v = point\n",
    "            kps.append([(x-600)/600, (y-600)/600])\n",
    "        #kps = [(x, y) for point in points for x, y, v in point]\n",
    "        #kps.reshape(1, 8)\n",
    "        #print(kps)\n",
    "        #print(images[0].numpy())\n",
    "        images = cv2.resize(images[0].numpy().transpose(1,2,0), (600, 600))\n",
    "        #print(images)\n",
    "        images = F.to_tensor(images).cuda()\n",
    "        labels = torch.as_tensor(kps).cuda()\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model_self(images).to(device)\n",
    "        #print(outputs)\n",
    "        labels = labels.view(-1, 18)\n",
    "        #print(labels)\n",
    "        loss = torch.sqrt(criterion(outputs, labels))\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print Epoch, Step and Loss value.\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(data_loader_train), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf606c-5f38-4814-b91f-672d1ccbf100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def has_cyrillic(text):\n",
    "    return bool(re.search('[а-яА-Я]', text))\n",
    "\n",
    "path = 'C:/Users/User/Petr/Net_2/test.jpg'\n",
    "\n",
    "has_cyrillic(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440dfcc-92f6-4e93-a960-1f07eb342a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21823538-8413-43bf-9828-85142f449a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
