{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f1883d-da11-4503-8c0f-1bc4c3610fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ca7b74-2118-4ffa-81cf-0087951f8b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Petr\\Anaconda\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_segmentation_masks\n",
    "import cv2\n",
    "from torchvision.transforms import functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import pandas as pd\n",
    "import MyUtils.Dataset\n",
    "from MyUtils.visualize import visualize_masks\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from MyUtils import transforms, utils, engine, train as transforms, utils, engine, train\n",
    "from MyUtils.utils import collate_fn\n",
    "from MyUtils.engine import train_one_epoch, evaluate\n",
    "from MyUtils.plot_statistic import plot_stats\n",
    "\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, maskrcnn_resnet50_fpn, MaskRCNN\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights, MaskRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import display, update_display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1867e543-59a1-4ad1-bbab-a44040938fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset classes is ['Asphalt road', 'Country road', 'Water']\n",
      "Class index is {'Asphalt road': 1, 'Country road': 2, 'Water': 3}\n",
      "Class index inverted is {1: 'Asphalt road', 2: 'Country road', 3: 'Water'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Classes', ylabel='Counts'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAw9UlEQVR4nO3deXRUVb728aeaDISQFKMpIzGAhHk0QRocIAJJowxODYgX8QouZIhGZLhIq4htUFSgBYelF0Gg6agoNu0AhCE0mEsbgiiTohgJSmJEYxIwJJDs949+OW2RgBACVWy/n7XOWpy99zn1O8kO9axdp6pcxhgjAAAAS/3O1wUAAACcT4QdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrBfi6AH9QUVGhgwcPKiwsTC6Xy9flAACAM2CMUXFxsSIjI/W73516/YawI+ngwYOKiorydRkAAKAaDhw4oCZNmpyyn7AjKSwsTNK/f1jh4eE+rgYAAJyJoqIiRUVFOc/jp0LYkZyXrsLDwwk7AABcZH7tFhRuUAYAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYLcDXBdgidtJiX5cAP5P19J2+LgEAIFZ2AACA5Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFjNp2Fn+vTpcrlcXpvH43H6jTGaPn26IiMjFRISol69emnXrl1e5ygtLVVSUpIaNWqk0NBQDRw4UN98882FvhQAAOCnfL6y065dO+Xm5jrbjh07nL5Zs2Zp9uzZmj9/vjIzM+XxeNS3b18VFxc7Y5KTk7VixQqlpqZq8+bNOnz4sPr376/y8nJfXA4AAPAzAT4vICDAazXnBGOM5s6dq2nTpumWW26RJL322muKiIjQsmXLNHr0aBUWFmrBggVasmSJ+vTpI0launSpoqKitHbtWiUmJl7QawEAAP7H5ys7X3zxhSIjI9WsWTMNHTpUX331lSQpOztbeXl5SkhIcMYGBwerZ8+eysjIkCRlZWXp2LFjXmMiIyPVvn17Z0xVSktLVVRU5LUBAAA7+TTsdOvWTYsXL9bq1av1yiuvKC8vTz169NAPP/ygvLw8SVJERITXMREREU5fXl6egoKCVL9+/VOOqcrMmTPldrudLSoqqoavDAAA+Aufhp1+/frp1ltvVYcOHdSnTx+99957kv79ctUJLpfL6xhjTKW2k/3amKlTp6qwsNDZDhw4cA5XAQAA/JnPX8b6pdDQUHXo0EFffPGFcx/PySs0+fn5zmqPx+NRWVmZCgoKTjmmKsHBwQoPD/faAACAnfwq7JSWlmrPnj269NJL1axZM3k8HqWlpTn9ZWVl2rhxo3r06CFJio2NVWBgoNeY3Nxc7dy50xkDAAB+23z6bqyJEydqwIABuvzyy5Wfn68///nPKioq0ogRI+RyuZScnKyUlBTFxMQoJiZGKSkpqlOnjoYNGyZJcrvdGjlypB588EE1bNhQDRo00MSJE52XxQAAAHwadr755hvdfvvtOnTokBo3bqzf//732rJli6KjoyVJkydPVklJicaOHauCggJ169ZNa9asUVhYmHOOOXPmKCAgQIMHD1ZJSYl69+6tRYsWqVatWr66LAAA4Edcxhjj6yJ8raioSG63W4WFhdW+fyd20uIargoXu6yn7/R1CQBgtTN9/vare3YAAABqGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACs5jdhZ+bMmXK5XEpOTnbajDGaPn26IiMjFRISol69emnXrl1ex5WWliopKUmNGjVSaGioBg4cqG+++eYCVw8AAPyVX4SdzMxMvfzyy+rYsaNX+6xZszR79mzNnz9fmZmZ8ng86tu3r4qLi50xycnJWrFihVJTU7V582YdPnxY/fv3V3l5+YW+DAAA4Id8HnYOHz6sO+64Q6+88orq16/vtBtjNHfuXE2bNk233HKL2rdvr9dee00///yzli1bJkkqLCzUggUL9Oyzz6pPnz7q0qWLli5dqh07dmjt2rW+uiQAAOBHfB52xo0bpxtvvFF9+vTxas/OzlZeXp4SEhKctuDgYPXs2VMZGRmSpKysLB07dsxrTGRkpNq3b++MqUppaamKioq8NgAAYKcAXz54amqqtm3bpszMzEp9eXl5kqSIiAiv9oiICO3fv98ZExQU5LUidGLMieOrMnPmTD322GPnWj4AALgI+Gxl58CBA7r//vu1dOlS1a5d+5TjXC6X174xplLbyX5tzNSpU1VYWOhsBw4cOLviAQDARcNnYScrK0v5+fmKjY1VQECAAgICtHHjRj333HMKCAhwVnROXqHJz893+jwej8rKylRQUHDKMVUJDg5WeHi41wYAAOzks7DTu3dv7dixQ9u3b3e2uLg43XHHHdq+fbuaN28uj8ejtLQ055iysjJt3LhRPXr0kCTFxsYqMDDQa0xubq527tzpjAEAAL9tPrtnJywsTO3bt/dqCw0NVcOGDZ325ORkpaSkKCYmRjExMUpJSVGdOnU0bNgwSZLb7dbIkSP14IMPqmHDhmrQoIEmTpyoDh06VLrhGQAA/Db59AblXzN58mSVlJRo7NixKigoULdu3bRmzRqFhYU5Y+bMmaOAgAANHjxYJSUl6t27txYtWqRatWr5sHIAAOAvXMYY4+sifK2oqEhut1uFhYXVvn8ndtLiGq4KF7usp+/0dQkAYLUzff72+efsAAAAnE+EHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwWrXCzrZt27Rjxw5n/+9//7tuuukmPfTQQyorK6ux4gAAAM5VtcLO6NGjtXfvXknSV199paFDh6pOnTp68803NXny5BotEAAA4FxUK+zs3btXnTt3liS9+eabuu6667Rs2TItWrRIb731Vk3WBwAAcE6qFXaMMaqoqJAkrV27VjfccIMkKSoqSocOHaq56gAAAM5RtcJOXFyc/vznP2vJkiXauHGjbrzxRklSdna2IiIiarRAAACAc1GtsDNnzhxt27ZN48eP17Rp09SiRQtJ0vLly9WjR48aLRAAAOBcBFTnoE6dOnm9G+uEp59+WgEB1TolAADAeVGtlZ3mzZvrhx9+qNR+9OhRtWzZ8pyLAgAAqCnVCjtff/21ysvLK7WXlpbqm2++OeeiAAAAaspZvea0cuVK59+rV6+W2+129svLy7Vu3To1a9as5qoDAAA4R2cVdm666SZJksvl0ogRI7z6AgMD1bRpUz377LM1VhwAAMC5Oquwc+KzdZo1a6bMzEw1atTovBQFAABQU6r11qns7OyargMAAOC8qPb7xNetW6d169YpPz/fWfE54dVXXz3nwgAAAGpCtcLOY489phkzZiguLk6XXnqpXC5XTdcFAABQI6oVdl566SUtWrRIw4cPr+l6AAAAalS1PmenrKysRr4W4sUXX1THjh0VHh6u8PBwde/eXR988IHTb4zR9OnTFRkZqZCQEPXq1Uu7du3yOkdpaamSkpLUqFEjhYaGauDAgXzWDwAAcFQr7IwaNUrLli075wdv0qSJnnzySW3dulVbt27V9ddfr0GDBjmBZtasWZo9e7bmz5+vzMxMeTwe9e3bV8XFxc45kpOTtWLFCqWmpmrz5s06fPiw+vfvX+WHHgIAgN8elzHGnO1B999/vxYvXqyOHTuqY8eOCgwM9OqfPXt2tQtq0KCBnn76ad19992KjIxUcnKypkyZIunfqzgRERF66qmnNHr0aBUWFqpx48ZasmSJhgwZIkk6ePCgoqKi9P777ysxMfGMHrOoqEhut1uFhYUKDw+vVt2xkxZX6zjYK+vpO31dAgBY7Uyfv6t1z86nn36qzp07S5J27tzp1Vfdm5XLy8v15ptv6siRI+revbuys7OVl5enhIQEZ0xwcLB69uypjIwMjR49WllZWTp27JjXmMjISLVv314ZGRmnDDulpaUqLS119ouKiqpVMwAA8H/VCjsbNmyosQJ27Nih7t276+jRo6pbt65WrFihtm3bKiMjQ5IUERHhNT4iIkL79++XJOXl5SkoKEj169evNCYvL++Ujzlz5kw99thjNXYNAADAf1Xrnp2a1KpVK23fvl1btmzRmDFjNGLECO3evdvpP3mlyBjzq6tHvzZm6tSpKiwsdLYDBw6c20UAAAC/Va2Vnfj4+NOGifXr15/xuYKCgtSiRQtJUlxcnDIzM/WXv/zFuU8nLy9Pl156qTM+Pz/fWe3xeDwqKytTQUGB1+pOfn7+ad8tFhwcrODg4DOuEQAAXLyqtbLTuXNnderUydnatm2rsrIybdu2TR06dDingowxKi0tVbNmzeTxeJSWlub0lZWVaePGjU6QiY2NVWBgoNeY3Nxc7dy5s0beGg8AAC5+1VrZmTNnTpXt06dP1+HDh8/4PA899JD69eunqKgoFRcXKzU1Venp6Vq1apVcLpeSk5OVkpKimJgYxcTEKCUlRXXq1NGwYcMkSW63WyNHjtSDDz6ohg0bqkGDBpo4caI6dOigPn36VOfSAACAZar93VhV+a//+i9dddVVeuaZZ85o/Hfffafhw4crNzdXbrdbHTt21KpVq9S3b19J0uTJk1VSUqKxY8eqoKBA3bp105o1axQWFuacY86cOQoICNDgwYNVUlKi3r17a9GiRapVq1ZNXhoAALhIVetzdk5lyZIlmjJlig4ePFhTp7wg+JwdnA98zg4AnF/n9XN2brnlFq99Y4xyc3O1detWPfzww9U5JQAAwHlRrbDjdru99n/3u9+pVatWmjFjhtcH/AEAAPhatcLOwoULa7oOAACA8+KcblDOysrSnj175HK51LZtW3Xp0qWm6gIAAKgR1Qo7+fn5Gjp0qNLT01WvXj0ZY1RYWKj4+HilpqaqcePGNV0nAABAtVTrQwWTkpJUVFSkXbt26ccff1RBQYF27typoqIi3XfffTVdIwAAQLVVa2Vn1apVWrt2rdq0aeO0tW3bVs8//zw3KAMAAL9SrZWdiooKBQYGVmoPDAxURUXFORcFAABQU6oVdq6//nrdf//9Xh8e+O233+qBBx5Q7969a6w4AACAc1WtsDN//nwVFxeradOmuuKKK9SiRQs1a9ZMxcXFmjdvXk3XCAAAUG3VumcnKipK27ZtU1pamj777DMZY9S2bVu+fBMAAPids1rZWb9+vdq2bauioiJJUt++fZWUlKT77rtPXbt2Vbt27bRp06bzUigAAEB1nFXYmTt3ru65554qv2zL7XZr9OjRmj17do0VBwAAcK7OKux88skn+sMf/nDK/oSEBGVlZZ1zUQAAADXlrMLOd999V+Vbzk8ICAjQ999/f85FAQAA1JSzCjuXXXaZduzYccr+Tz/9VJdeeuk5FwUAAFBTzirs3HDDDXrkkUd09OjRSn0lJSV69NFH1b9//xorDgAA4Fyd1VvP//SnP+ntt99Wy5YtNX78eLVq1Uoul0t79uzR888/r/Lyck2bNu181QoAAHDWzirsREREKCMjQ2PGjNHUqVNljJEkuVwuJSYm6oUXXlBERMR5KRQAAKA6zvpDBaOjo/X++++roKBAX375pYwxiomJUf369c9HfQAAAOekWp+gLEn169dX165da7IWAACAGlet78YCAAC4WBB2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYzadhZ+bMmeratavCwsJ0ySWX6KabbtLnn3/uNcYYo+nTpysyMlIhISHq1auXdu3a5TWmtLRUSUlJatSokUJDQzVw4EB98803F/JSAACAn/Jp2Nm4caPGjRunLVu2KC0tTcePH1dCQoKOHDnijJk1a5Zmz56t+fPnKzMzUx6PR3379lVxcbEzJjk5WStWrFBqaqo2b96sw4cPq3///iovL/fFZQEAAD/iMsYYXxdxwvfff69LLrlEGzdu1HXXXSdjjCIjI5WcnKwpU6ZI+vcqTkREhJ566imNHj1ahYWFaty4sZYsWaIhQ4ZIkg4ePKioqCi9//77SkxM/NXHLSoqktvtVmFhocLDw6tVe+ykxdU6DvbKevpOX5cAAFY70+dvv7pnp7CwUJLUoEEDSVJ2drby8vKUkJDgjAkODlbPnj2VkZEhScrKytKxY8e8xkRGRqp9+/bOmJOVlpaqqKjIawMAAHbym7BjjNGECRN0zTXXqH379pKkvLw8SVJERITX2IiICKcvLy9PQUFBql+//inHnGzmzJlyu93OFhUVVdOXAwAA/ITfhJ3x48fr008/1d/+9rdKfS6Xy2vfGFOp7WSnGzN16lQVFhY624EDB6pfOAAA8Gt+EXaSkpK0cuVKbdiwQU2aNHHaPR6PJFVaocnPz3dWezwej8rKylRQUHDKMScLDg5WeHi41wYAAOwU4MsHN8YoKSlJK1asUHp6upo1a+bV36xZM3k8HqWlpalLly6SpLKyMm3cuFFPPfWUJCk2NlaBgYFKS0vT4MGDJUm5ubnauXOnZs2adWEvCPAzOTM6+LoE+JHLH9nh6xIAn/Bp2Bk3bpyWLVumv//97woLC3NWcNxut0JCQuRyuZScnKyUlBTFxMQoJiZGKSkpqlOnjoYNG+aMHTlypB588EE1bNhQDRo00MSJE9WhQwf16dPHl5cHAAD8gE/DzosvvihJ6tWrl1f7woULddddd0mSJk+erJKSEo0dO1YFBQXq1q2b1qxZo7CwMGf8nDlzFBAQoMGDB6ukpES9e/fWokWLVKtWrQt1KQAAwE/51efs+Aqfs4PzwR8+Z4eXsfBLvIwF21yUn7MDAABQ0wg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNZ+GnX/+858aMGCAIiMj5XK59M4773j1G2M0ffp0RUZGKiQkRL169dKuXbu8xpSWliopKUmNGjVSaGioBg4cqG+++eYCXgUAAPBnPg07R44cUadOnTR//vwq+2fNmqXZs2dr/vz5yszMlMfjUd++fVVcXOyMSU5O1ooVK5SamqrNmzfr8OHD6t+/v8rLyy/UZQAAAD8W4MsH79evn/r161dlnzFGc+fO1bRp03TLLbdIkl577TVFRERo2bJlGj16tAoLC7VgwQItWbJEffr0kSQtXbpUUVFRWrt2rRITEy/YtQAAAP/kt/fsZGdnKy8vTwkJCU5bcHCwevbsqYyMDElSVlaWjh075jUmMjJS7du3d8ZUpbS0VEVFRV4bAACwk9+Gnby8PElSRESEV3tERITTl5eXp6CgINWvX/+UY6oyc+ZMud1uZ4uKiqrh6gEAgL/w27Bzgsvl8to3xlRqO9mvjZk6daoKCwud7cCBAzVSKwAA8D9+G3Y8Ho8kVVqhyc/Pd1Z7PB6PysrKVFBQcMoxVQkODlZ4eLjXBgAA7OS3YadZs2byeDxKS0tz2srKyrRx40b16NFDkhQbG6vAwECvMbm5udq5c6czBgAA/Lb59N1Yhw8f1pdffunsZ2dna/v27WrQoIEuv/xyJScnKyUlRTExMYqJiVFKSorq1KmjYcOGSZLcbrdGjhypBx98UA0bNlSDBg00ceJEdejQwXl3FgAA+G3zadjZunWr4uPjnf0JEyZIkkaMGKFFixZp8uTJKikp0dixY1VQUKBu3bppzZo1CgsLc46ZM2eOAgICNHjwYJWUlKh3795atGiRatWqdcGvBwAA+B+XMcb4ughfKyoqktvtVmFhYbXv34mdtLiGq8LFLuvpO31dgnJmdPB1CfAjlz+yw9clADXqTJ+//faeHQAAgJpA2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAq/n0i0ABAL8tV8+72tclwI98mPThBXkcVnYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgNcIOAACwGmEHAABYjbADAACsRtgBAABWI+wAAACrEXYAAIDVCDsAAMBqhB0AAGA1wg4AALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAVrMm7Lzwwgtq1qyZateurdjYWG3atMnXJQEAAD9gRdh5/fXXlZycrGnTpunjjz/Wtddeq379+iknJ8fXpQEAAB+zIuzMnj1bI0eO1KhRo9SmTRvNnTtXUVFRevHFF31dGgAA8LEAXxdwrsrKypSVlaX/+Z//8WpPSEhQRkZGlceUlpaqtLTU2S8sLJQkFRUVVbuO8tKSah8LO53LfKopxUfLfV0C/Ig/zMnjJcd9XQL8yLnOyRPHG2NOO+6iDzuHDh1SeXm5IiIivNojIiKUl5dX5TEzZ87UY489Vqk9KirqvNSI3yb3vHt9XQLgbabb1xUAXtxTamZOFhcXy+0+9bku+rBzgsvl8to3xlRqO2Hq1KmaMGGCs19RUaEff/xRDRs2POUxODNFRUWKiorSgQMHFB4e7utyAOYk/A5zsuYYY1RcXKzIyMjTjrvow06jRo1Uq1atSqs4+fn5lVZ7TggODlZwcLBXW7169c5Xib9J4eHh/BHDrzAn4W+YkzXjdCs6J1z0NygHBQUpNjZWaWlpXu1paWnq0aOHj6oCAAD+4qJf2ZGkCRMmaPjw4YqLi1P37t318ssvKycnR/feyz0TAAD81lkRdoYMGaIffvhBM2bMUG5urtq3b6/3339f0dHRvi7tNyc4OFiPPvpopZcJAV9hTsLfMCcvPJf5tfdrAQAAXMQu+nt2AAAAToewAwAArEbYAQAAViPs4Ddr+vTp6ty5s6/LwGmkp6fL5XLpp59+OqfzNG3aVHPnzq2Rms4X5iNw/hB2LgJ5eXlKSkpS8+bNFRwcrKioKA0YMEDr1q274LW4XC698847F/xx4b8yMjJUq1Yt/eEPf/B1KWeMeYyz9dJLLyksLEzHj//nu70OHz6swMBAXXvttV5jN23aJJfLpb179572nDUV5vHrCDt+7uuvv1ZsbKzWr1+vWbNmaceOHVq1apXi4+M1btw4X5dXpWPHjvnluXB+vPrqq0pKStLmzZuVk5Pj63LOK+bjb1d8fLwOHz6srVu3Om2bNm2Sx+NRZmamfv75Z6c9PT1dkZGRatmy5QWpzRjjFcJQGWHHz40dO1Yul0sfffSRbrvtNrVs2VLt2rXThAkTtGXLFmdcTk6OBg0apLp16yo8PFyDBw/Wd9995/Tfdddduummm7zOnZycrF69ejn7vXr10n333afJkyerQYMG8ng8mj59utPftGlTSdLNN98sl8vl7J9Yfn/11Ved1afXXntNDRs29Pp2eUm69dZbdeedd1Z5rV9//bVcLpfeeOMN9erVS7Vr19bSpUtVUVGhGTNmqEmTJgoODlbnzp21atUqr2OnTJmili1bqk6dOmrevLkefvjhSk9MTz75pCIiIhQWFqaRI0fq6NGjp/vR4wwcOXJEb7zxhsaMGaP+/ftr0aJFXv0FBQW644471LhxY4WEhCgmJkYLFy6U9J/fd2pqqnr06KHatWurXbt2Sk9Pr/Q4WVlZiouLU506ddSjRw99/vnnTt++ffs0aNAgRUREqG7duuratavWrl17yppPNY9PxnzEL7Vq1UqRkZFe8zM9PV2DBg3SFVdcoYyMDK/2+Ph4LV26VHFxcQoLC5PH49GwYcOUn58v6d/zKz4+XpJUv359uVwu3XXXXZL+HV5mzZql5s2bKyQkRJ06ddLy5cu9zu9yubR69WrFxcUpODhYmzZtOv8/hIuZgd/64YcfjMvlMikpKacdV1FRYbp06WKuueYas3XrVrNlyxZz5ZVXmp49ezpjRowYYQYNGuR13P333+81pmfPniY8PNxMnz7d7N2717z22mvG5XKZNWvWGGOMyc/PN5LMwoULTW5ursnPzzfGGPPoo4+a0NBQk5iYaLZt22Y++eQT8/PPPxu3223eeOMN5/zff/+9CQoKMuvXr6/yOrKzs40k07RpU/PWW2+Zr776ynz77bdm9uzZJjw83Pztb38zn332mZk8ebIJDAw0e/fudY59/PHHzYcffmiys7PNypUrTUREhHnqqaec/tdff90EBQWZV155xXz22Wdm2rRpJiwszHTq1Om0P1uc3oIFC0xcXJwxxph//OMfpmnTpqaiosLpHzdunOncubPJzMw02dnZJi0tzaxcudIY85/fd5MmTczy5cvN7t27zahRo0xYWJg5dOiQMcaYDRs2GEmmW7duJj093ezatctce+21pkePHs5jbN++3bz00kvm008/NXv37jXTpk0ztWvXNvv373fGREdHmzlz5hhjTj2PT8Z8xMmGDRtmEhISnP2uXbuaN99804wZM8Y89NBDxhhjSktLTUhIiPnf//1fs2DBAvP++++bffv2mf/7v/8zv//9702/fv2MMcYcP37cvPXWW0aS+fzzz01ubq756aefjDHGPPTQQ6Z169Zm1apVZt++fWbhwoUmODjYpKenG2P+83fRsWNHs2bNGvPll186fzOoGmHHj/3rX/8ykszbb7992nFr1qwxtWrVMjk5OU7brl27jCTz0UcfGWPOPOxcc801XmO6du1qpkyZ4uxLMitWrPAa8+ijj5rAwMBKTxpjxoxx/rCNMWbu3LmmefPmXk+Gv3TiyWXu3Lle7ZGRkeaJJ56oVNfYsWOrPI8xxsyaNcvExsY6+927dzf33nuv15hu3brx5HKOevTo4fy+jh07Zho1amTS0tKc/gEDBpj//u//rvLYE7/vJ5980mk7duyYadKkiRMMTvynvnbtWmfMe++9ZySZkpKSU9bVtm1bM2/ePGf/l2HHmKrn8anqYz7ihJdfftmEhoaaY8eOmaKiIhMQEGC+++47k5qa6gTwjRs3Gklm3759lY7/6KOPjCRTXFxsjPnP/C4oKHDGHD582NSuXdtkZGR4HTty5Ehz++23ex33zjvvnKcrtQ8vY/kx8/8/3Nrlcp123J49exQVFaWoqCinrW3btqpXr5727NlzVo/ZsWNHr/1LL73UWXY9nejoaDVu3Nir7Z577tGaNWv07bffSpIWLlyou+6661evJy4uzvl3UVGRDh48qKuvvtprzNVXX+11bcuXL9c111wjj8ejunXr6uGHH/a6f2TPnj3q3r271zlO3sfZ+fzzz/XRRx9p6NChkqSAgAANGTJEr776qjNmzJgxSk1NVefOnTV58mSvpf4Tfvl7CAgIUFxcXKV5+8t5eemll0qSMy+PHDmiyZMnO3O+bt26+uyzz2rs/iHmI06Ij4/XkSNHlJmZqU2bNqlly5a65JJL1LNnT2VmZurIkSNKT0/X5ZdfrubNm+vjjz/WoEGDFB0drbCwMOe2gdPNzd27d+vo0aPq27ev6tat62yLFy/Wvn37vMb+cm7i9Kz4bixbxcTEyOVyac+ePZXut/klY0yVAeKX7b/73e+c8HRCVTdbBgYGeu27XC5VVFT8aq2hoaGV2rp06aJOnTpp8eLFSkxM1I4dO/SPf/yjWuc6+fp+eW1btmzR0KFD9dhjjykxMVFut1upqal69tlnf/WxUH0LFizQ8ePHddlllzltxhgFBgaqoKBA9evXV79+/bR//3699957Wrt2rXr37q1x48bpmWeeOe25T/59/3Jenug7MS8nTZqk1atX65lnnlGLFi0UEhKi2267TWVlZTVyncxHnNCiRQs1adJEGzZsUEFBgXr27ClJ8ng8atasmT788ENt2LBB119/vY4cOaKEhAQlJCRo6dKlaty4sXJycpSYmHjauXliXr/33ntef1uSKn2XVlVzE1VjZcePNWjQQImJiXr++ed15MiRSv0n3q7Ytm1b5eTk6MCBA07f7t27VVhYqDZt2kiSGjdurNzcXK/jt2/fftY1BQYGqry8/IzHjxo1SgsXLtSrr76qPn36eK0+nYnw8HBFRkZq8+bNXu0ZGRnOtX344YeKjo7WtGnTFBcXp5iYGO3fv99rfJs2bbxu6JZUaR9n7vjx41q8eLGeffZZbd++3dk++eQTRUdH669//asztnHjxrrrrru0dOlSzZ07Vy+//LLXuX75ezh+/LiysrLUunXrM65l06ZNuuuuu3TzzTerQ4cO8ng8+vrrr097zNnO4xOYj4iPj1d6errS09O93uDRs2dPrV69Wlu2bFF8fLw+++wzHTp0SE8++aSuvfZatW7dutIqeVBQkCR5zcW2bdsqODhYOTk5atGihdd2tv9/4j9Y2fFzL7zwgnr06KGrrrpKM2bMUMeOHXX8+HGlpaXpxRdf1J49e9SnTx917NhRd9xxh+bOnavjx49r7Nix6tmzp7PMef311+vpp5/W4sWL1b17dy1dulQ7d+5Uly5dzqqepk2bat26dbr66qsVHBys+vXrn3b8HXfcoYkTJ+qVV17R4sWLq/UzmDRpkh599FFdccUV6ty5sxYuXKjt27c7T6gtWrRQTk6OUlNT1bVrV7333ntasWKF1znuv/9+jRgxQnFxcbrmmmv017/+Vbt27VLz5s2rVdNv3bvvvquCggKNHDlSbrfbq++2227TggULNH78eD3yyCOKjY1Vu3btVFpaqnfffdcJBSc8//zziomJUZs2bTRnzhwVFBTo7rvvPuNaWrRoobffflsDBgyQy+XSww8//KurkWc7j3+J+fjbduJjP44dO+as7Ej/DjtjxozR0aNHFR8fr9q1aysoKEjz5s3Tvffeq507d+rxxx/3Old0dLRcLpfeffdd3XDDDQoJCVFYWJgmTpyoBx54QBUVFbrmmmtUVFSkjIwM1a1bVyNGjLjQl2wHH94vhDN08OBBM27cOBMdHW2CgoLMZZddZgYOHGg2bNjgjNm/f78ZOHCgCQ0NNWFhYeaPf/yjycvL8zrPI488YiIiIozb7TYPPPCAGT9+fKUblO+//36vYwYNGmRGjBjh7K9cudK0aNHCBAQEmOjoaGPMv29QPt2NlcOHDzcNGjQwR48ePe11nrgh9OOPP/ZqLy8vN4899pi57LLLTGBgoOnUqZP54IMPvMZMmjTJNGzY0NStW9cMGTLEzJkzx7jdbq8xTzzxhGnUqJGpW7euGTFihJk8eTI3hFZT//79zQ033FBlX1ZWlpFksrKyzOOPP27atGljQkJCTIMGDcygQYPMV199ZYz5z+972bJlplu3biYoKMi0adPGrFu3zjlXVTdwfvzxx0aSyc7Ods4THx9vQkJCTFRUlJk/f36luXzyDcpVzeOTMR9RlRPzonXr1l7tBw4cMJLMFVdc4bQtW7bMNG3a1AQHB5vu3bublStXVppTM2bMMB6Px7hcLuf/2oqKCvOXv/zFtGrVygQGBprGjRubxMREs3HjRmNM1X8XOD2XMSfdyAHUsL59+6pNmzZ67rnnfF0K/MjXX3+tZs2a6eOPP+ZrEgCcV7yMhfPmxx9/1Jo1a7R+/XrNnz/f1+UAAH6jCDs4b6688koVFBToqaeeUqtWrXxdDgDgN4qXsQAAgNV46zkAALAaYQcAAFiNsAMAAKxG2AEAAFYj7AAAAKsRdgBcFFwul9555x1flwHgIkTYAeAX8vLylJSUpObNmys4OFhRUVEaMGCA1q1b5+vSAFzk+FBBAD739ddf6+qrr1a9evU0a9YsdezYUceOHdPq1as1btw4ffbZZ74uEcBFjJUdAD43duxYuVwuffTRR7rtttvUsmVLtWvXThMmTNCWLVuqPGbKlClq2bKl6tSpo+bNm+vhhx/WsWPHnP5PPvlE8fHxCgsLU3h4uGJjY7V161ZJ0v79+zVgwADVr19foaGhateund5//33n2N27d+uGG25Q3bp1FRERoeHDh+vQoUNO//Lly9WhQweFhISoYcOG6tOnj44cOXKefjoAzhUrOwB86scff9SqVav0xBNPKDQ0tFJ/vXr1qjwuLCxMixYtUmRkpHbs2KF77rlHYWFhmjx5siTpjjvuUJcuXfTiiy+qVq1a2r59uwIDAyVJ48aNU1lZmf75z38qNDRUu3fvVt26dSVJubm56tmzp+655x7Nnj1bJSUlmjJligYPHqz169crNzdXt99+u2bNmqWbb75ZxcXF2rRpk/gwesB/EXYA+NSXX34pY4xat259Vsf96U9/cv7dtGlTPfjgg3r99dedsJOTk6NJkyY5542JiXHG5+Tk6NZbb1WHDh0kSc2bN3f6XnzxRV155ZVKSUlx2l599VVFRUVp7969Onz4sI4fP65bbrlF0dHRkuScB4B/IuwA8KkTKyIul+usjlu+fLnmzp2rL7/80gkg4eHhTv+ECRM0atQoLVmyRH369NEf//hHXXHFFZKk++67T2PGjNGaNWvUp08f3XrrrerYsaMkKSsrSxs2bHBWen5p3759SkhIUO/evdWhQwclJiYqISFBt912m+rXr1/dHwGA84x7dgD4VExMjFwul/bs2XPGx2zZskVDhw5Vv3799O677+rjjz/WtGnTVFZW5oyZPn26du3apRtvvFHr169X27ZttWLFCknSqFGj9NVXX2n48OHasWOH4uLiNG/ePElSRUWFBgwYoO3bt3ttX3zxha677jrVqlVLaWlp+uCDD9S2bVvNmzdPrVq1UnZ2ds3+YADUGL71HIDP9evXTzt27NDnn39e6b6dn376SfXq1ZPL5dKKFSt000036dlnn9ULL7ygffv2OeNGjRql5cuX66effqryMW6//XYdOXJEK1eurNQ3depUvffee/r00081bdo0vfXWW9q5c6cCAn598bu8vFzR0dGaMGGCJkyYcHYXDuCCYGUHgM+98MILKi8v11VXXaW33npLX3zxhfbs2aPnnntO3bt3rzS+RYsWysnJUWpqqvbt26fnnnvOWbWRpJKSEo0fP17p6enav3+/PvzwQ2VmZqpNmzaSpOTkZK1evVrZ2dnatm2b1q9f7/SNGzdOP/74o26//XZ99NFH+uqrr7RmzRrdfffdKi8v17/+9S+lpKRo69atysnJ0dtvv63vv//eOR6AHzIA4AcOHjxoxo0bZ6Kjo01QUJC57LLLzMCBA82GDRuMMcZIMitWrHDGT5o0yTRs2NDUrVvXDBkyxMyZM8e43W5jjDGlpaVm6NChJioqygQFBZnIyEgzfvx4U1JSYowxZvz48eaKK64wwcHBpnHjxmb48OHm0KFDzrn37t1rbr75ZlOvXj0TEhJiWrdubZKTk01FRYXZvXu3SUxMNI0bNzbBwcGmZcuWZt68eRfqxwSgGngZCwAAWI2XsQAAgNUIOwAAwGqEHQAAYDXCDgAAsBphBwAAWI2wAwAArEbYAQAAViPsAAAAqxF2AACA1Qg7AADAaoQdAABgtf8HFR2Du1u1nQkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "path = 'C:/Users/User/Petr/Net_4/Dataset_4/'\n",
    "visualise_path = 'C:/Users/User/Petr/Net_4/Visualize/'\n",
    "\n",
    "obj_colors = {\n",
    "    'Country road': (255, 136, 0),\n",
    "    'Asphalt road': (0, 0, 0),\n",
    "    'Water': (0, 0, 255),\n",
    "    'Road': (255, 136, 0)\n",
    "}\n",
    "\n",
    "classes = list(set([mask.split('-')[-2] for mask in os.listdir(path + 'masks/') if mask.endswith('.npy')]))\n",
    "\n",
    "classes.sort()\n",
    "\n",
    "class_index = {cls: index + 1 for index, cls in enumerate(classes)}\n",
    "\n",
    "inv_classes = {v: k for k, v in class_index.items()}\n",
    "\n",
    "\n",
    "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "weights_v2 = MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "\n",
    "print(f'Dataset classes is {classes}')\n",
    "print(f'Class index is {class_index}')\n",
    "print(f'Class index inverted is {inv_classes}')\n",
    "\n",
    "dataset_stats = defaultdict(int)\n",
    "\n",
    "for mask in os.listdir(path + 'masks/'):\n",
    "    if mask.endswith('.npy'):\n",
    "        dataset_stats[mask.split('-')[-2]] += 1\n",
    "\n",
    "df = pd.DataFrame(data=dataset_stats.items(), columns=['Classes', 'Counts'])\n",
    "\n",
    "sns.barplot(data=df, x='Classes', y='Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7731f65b-e7b1-456e-a216-2ce97752db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = 'C:/Users/User/Petr/Net_4/Dataset/'\n",
    "\n",
    "FDA_source = {\n",
    "    'reference_images': [old_path + 'images/mmih-3-3.jpg', old_path + 'images/B.protopop-2-2.jpg', old_path + 'images/malin-2-2.jpg', old_path + 'images/TEO_1_00515.JPG'],\n",
    "    'read_fn': lambda x: cv2.cvtColor(cv2.imread(x), cv2.COLOR_BGR2RGB),\n",
    "    'p': 0.3\n",
    "}\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    #A.RandomBrightnessContrast(p=0.5),\n",
    "    #A.RandomSizedBBoxSafeCrop(height=1000, width=1000, erosion_rate=0.0, interpolation=1, always_apply=False, p=0.3),\n",
    "    #A.ToSepia(always_apply=False, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RGBShift(r_shift_limit=(-10, 10), g_shift_limit=(-10, 10), b_shift_limit=(-10, 10), always_apply=False, p=0.1),\n",
    "        A.GaussNoise(p=0.1)\n",
    "    ], p=1.0),\n",
    "    #A.RandomGravel(gravel_roi=(0.1, 0.1, 0.9, 0.9), number_of_patches=2, always_apply=False, p=0.5),\n",
    "    #A.RandomShadow(shadow_roi=(0, 0, 1, 1), always_apply=False, p=0.3),\n",
    "    #A.Solarize(threshold=(200, 200), always_apply=False, p=0.5),\n",
    "    #A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, brightness_coeff=2.5, always_apply=False, p=0.3),\n",
    "    #A.CropNonEmptyMaskIfExists(height=3000, width=3000, ignore_values=None, ignore_channels=None, always_apply=False, p=0.5),\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b9f9e5-8fc3-44b1-b210-1e494bb824d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    }
   ],
   "source": [
    "dataset_train_initial = MyUtils.Dataset.RoadsDataset(root=path, class_index=class_index, transform=transform)\n",
    "dataset_test_initial = MyUtils.Dataset.RoadsDataset(root=path, class_index=class_index, transform=None)\n",
    "\n",
    "print(len(dataset_train_initial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc20f3-ce82-4388-b7ee-87554863822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = dataset_train_initial[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89652f7b-6e28-470e-817f-1160d047d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_masks(image, target, inv_classes, obj_colors=obj_colors, show=True, alpha=0.7)\n",
    "#visualize_masks(image, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917c1729-d243-42d1-ab94-38912f6af939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(mask_logits, proposals, gt_masks, gt_labels, mask_matched_idxs):\n",
    "    # type: (Tensor, List[Tensor], List[Tensor], List[Tensor], List[Tensor]) -> Tensor\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        proposals (list[BoxList])\n",
    "        mask_logits (Tensor)\n",
    "        targets (list[BoxList])\n",
    "\n",
    "    Return:\n",
    "        mask_loss (Tensor): scalar tensor containing the loss\n",
    "    \"\"\"\n",
    "\n",
    "    #print('New loss is working')\n",
    "\n",
    "    discretization_size = mask_logits.shape[-1]\n",
    "    labels = [gt_label[idxs] for gt_label, idxs in zip(gt_labels, mask_matched_idxs)]\n",
    "    mask_targets = [\n",
    "        torchvision.models.detection.roi_heads.project_masks_on_boxes(m, p, i, discretization_size) for m, p, i in zip(gt_masks, proposals, mask_matched_idxs)\n",
    "    ]\n",
    "\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    mask_targets = torch.cat(mask_targets, dim=0)\n",
    "    classes = mask_targets.shape[-1]\n",
    "    pos_weight = torch.full((classes,), 3).to(labels.device)\n",
    "    #print(mask_targets.shape)\n",
    "\n",
    "    # torch.mean (in binary_cross_entropy_with_logits) doesn't\n",
    "    # accept empty tensors, so handle it separately\n",
    "    if mask_targets.numel() == 0:\n",
    "        return mask_logits.sum() * 0\n",
    "\n",
    "    #print(mask_logits[torch.arange(labels.shape[0], device=labels.device), labels])\n",
    "    #print(mask_targets)\n",
    "\n",
    "    mask_loss = torch.nn.functional.binary_cross_entropy_with_logits(input=mask_logits[torch.arange(labels.shape[0], device=labels.device), labels], target=mask_targets, pos_weight=pos_weight) #pos_weight=pos_weight\n",
    "    return mask_loss\n",
    "\n",
    "#torchvision.models.detection.roi_heads.mask_rcnn_loss = my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce79841f-2c76-4a41-8fa3-57ac1aede546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_model(weights_path=None):\n",
    "    model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "\n",
    "# Get the numbner of output channels for the Mask Predictor\n",
    "    dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
    "\n",
    "# Replace the box predictor\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features_box, num_classes=len(classes))\n",
    "\n",
    "# Replace the mask predictor\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_channels=in_features_mask, dim_reduced=dim_reduced, num_classes=len(classes))\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)   \n",
    "\n",
    "# Set the model's device and data type\n",
    "    #model.to(device=device)\n",
    "\n",
    "# Add attributes to store the device and model name for later reference\n",
    "    #model.device = device\n",
    "    model.name = 'maskrcnn_resnet50_fpn_v2'\n",
    "    return model\n",
    "\n",
    "def get_mask_model_v2(weights_path=None):\n",
    "    model = maskrcnn_resnet50_fpn_v2(weights=weights_v2)\n",
    "    torchvision.models.detection.roi_heads.maskrcnn_loss = my_loss\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "\n",
    "# Get the numbner of output channels for the Mask Predictor\n",
    "    dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
    "\n",
    "# Replace the box predictor\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features_box, num_classes=len(classes) + 1)\n",
    "\n",
    "# Replace the mask predictor\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_channels=in_features_mask, dim_reduced=dim_reduced, num_classes=len(classes) + 1)\n",
    "\n",
    "    #model.roi_heads.maskrcnn_loss = my_loss\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        model.load_state_dict(state_dict)   \n",
    "\n",
    "# Set the model's device and data type\n",
    "    #model.to(device=device)\n",
    "\n",
    "# Add attributes to store the device and model name for later reference\n",
    "    #model.device = device\n",
    "    model.name = 'maskrcnn_resnet50_fpn_v2'\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689077d-66db-4ff1-862f-610237fcaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'C:/Users/User/Petr/Net_4/save_model'\n",
    "log_path = 'C:/Users/User/Petr/Net_4/Metric_log'\n",
    "\n",
    "indices = torch.randperm(len(dataset_train_initial)).tolist()\n",
    "thirty_pc = int(len(dataset_train_initial) * 0.15)\n",
    "dataset_train = torch.utils.data.Subset(dataset_train_initial, indices[:-thirty_pc])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test_initial, indices[-thirty_pc:])\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') #torch.device('cuda') if torch.cuda.is_available() else \n",
    "\n",
    "model = get_mask_model_v2() #weights_path=f'{save_path}/weights_20_loss.pth'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "    #params1 = [p for p in model.roi_heads.keypoint_head.parameters() if p.requires_grad]\n",
    "    \n",
    "optimizer = torch.optim.SGD(params, lr=1.0e-4, momentum=0.90, weight_decay=0.0001)#, weight_decay=0.0001\n",
    "    \n",
    "    #optimizer = torch.optim.SGD([{'params': params1},\n",
    "    #                             {'params': model.roi_heads.keypoint_predictor.parameters(), 'lr': .001},], lr=0.001, momentum=0.90)#, weight_decay=0.0001\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "num_epochs = 50\n",
    "\n",
    "bbox_stats = []\n",
    "masks_stats=[]\n",
    "\n",
    "loss_bb = []\n",
    "loss_masks = []\n",
    "loss = []\n",
    "\n",
    "start_from = 0\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    logger = train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=int(len(dataset_train) / 10))\n",
    "    lr_scheduler.step()\n",
    "    evaluator = evaluate(model, data_loader_test, device)\n",
    "\n",
    "    bbox_stats.append(evaluator.coco_eval['bbox'].stats[:6])\n",
    "    masks_stats.append(evaluator.coco_eval['segm'].stats[:6])\n",
    "\n",
    "    loss_masks.append(logger.meters['loss_mask'].global_avg)\n",
    "    loss_bb.append(logger.meters['loss_box_reg'].global_avg)\n",
    "    loss.append(logger.meters['loss'].global_avg)\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'{save_path}/test_{start_from + epoch+1}_loss.pth')\n",
    "        plot_stats(epoch + 1, bbox_stats, loss_bb, loss, masks_stats=masks_stats, loss_masks=loss_masks, num=epoch+1, log_path=log_path)#, num=num\n",
    "            \n",
    "plot_stats(num_epochs, bbox_stats, loss_bb, loss, masks_stats=masks_stats, loss_masks=loss_masks, num=epoch+1, show=True, log_path=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f136b-fbb4-4598-a4b3-ae38caf419d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{save_path}/weights_10_loss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f045d5d-769e-4d5c-a655-cfb8fb235a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 703/703 [19:40<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "test_path = 'C:/Users/User/Petr/Net_4/test_model/images/'\n",
    "out_path = 'C:/Users/User/Petr/Net_4/test_model/pred/'\n",
    "\n",
    "save_path = 'C:/Users/User/Petr/Net_4/save_model/'\n",
    "model = get_mask_model_v2(save_path + 'weights_40_loss.pth')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "threshold = 0.975\n",
    "mask_threshold = 0.6\n",
    "\n",
    "i = 0\n",
    "for image in tqdm(os.listdir(test_path)):\n",
    "    img = cv2.imdecode(np.fromfile(os.path.join(test_path, image), dtype=np.uint8), cv2.IMREAD_UNCHANGED) #cv2.imread(os.path.join(test_path, image))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img_orig = cv2.imdecode(np.fromfile(os.path.join(test_path, image), dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    img_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = F.to_tensor(img)\n",
    "    type(img)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        #model_test.to(device)\n",
    "        model.eval()\n",
    "        out = model([img,])\n",
    "        out = out[0]\n",
    "    #img = (img[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    scores_valid = out['scores'] > threshold\n",
    "    labels_valid = out['labels'] < 3\n",
    "\n",
    "    target = {}\n",
    "    target['masks'] = out['masks'][scores_valid * labels_valid]\n",
    "    target['boxes'] = out['boxes'][scores_valid * labels_valid]\n",
    "    #print(target['boxes'])\n",
    "    #print(target['masks'])\n",
    "    target['labels'] = out['labels'][scores_valid * labels_valid]\n",
    "    \n",
    "    #target['masks'] = out['masks'][labels_valid]\n",
    "    #print_contours(target, image)\n",
    "    #target['boxes'] = out['boxes'][labels_valid]\n",
    "    #target['labels'] = out['labels'][labels_valid]\n",
    "\n",
    "    img =  F.to_tensor(print_contours(target, img_orig))\n",
    "\n",
    "    visualize_masks(img.detach().cpu(), target=target, inv_classes=inv_classes, obj_colors=obj_colors, alpha=0.5, save_path=out_path + image, mask_threshold=mask_threshold) # .detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e3c79c-3ac2-4921-afd2-94f602861eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43859e96-db0e-4549-9e0a-5f3242027bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_contours(target, img):\n",
    "    if not len(target['masks']):\n",
    "        return img\n",
    "\n",
    "    label_masks_threshold = {1: 0.55, 2: 0.61, 3: 0.1}\n",
    "    masks = target['masks'] #> mask_threshold\n",
    "    boxes = target['boxes']\n",
    "    \n",
    "    masks = masks.squeeze(1)\n",
    "    #masks = masks.detach().cpu().numpy()\n",
    "    labels = set(target['labels'].detach().cpu().numpy())\n",
    "\n",
    "    for label in labels:\n",
    "        label_masks = target['labels'] == label\n",
    "\n",
    "        #print(label_masks)\n",
    "\n",
    "        label_boxes = boxes[label_masks]\n",
    "\n",
    "        label_boxes = label_boxes.detach().cpu().numpy()\n",
    "\n",
    "        label_masks = masks[label_masks] > label_masks_threshold[label]\n",
    "\n",
    "        label_masks = label_masks.detach().cpu().numpy()\n",
    "\n",
    "        total_mask = label_masks[0]\n",
    "\n",
    "        for i in range(1, label_masks.shape[0]):\n",
    "            total_mask = total_mask | label_masks[i]\n",
    "        \n",
    "        total_mask = total_mask.astype('uint8')\n",
    "        contours, heir = cv2.findContours(total_mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        areas = [cv2.contourArea(c) for c in contours]\n",
    "        try:\n",
    "            max_area = max(areas)\n",
    "        except ValueError:\n",
    "            print('areas is empty. skip.')\n",
    "        valid_contours = tuple([c for c in contours if cv2.contourArea(c) > max_area*0.5])\n",
    "        eps = 20\n",
    "        for contour in valid_contours:\n",
    "            c = np.squeeze(contour)\n",
    "            step = int(len(c) * 0.15)\n",
    "            for point in c[::step]:\n",
    "                skip_point = False\n",
    "                x, y = point\n",
    "                for box in label_boxes:\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    if (abs(x - x_min) <= eps or abs(x - x_max) <= eps) or (abs(y - y_min) <= eps or abs(y - y_max) <= eps):\n",
    "                        skip_point = True\n",
    "                        break\n",
    "                if skip_point:\n",
    "                    continue\n",
    "                img = cv2.circle(img.copy(), tuple(point), 5, (0,0,255), -1)\n",
    "        #print(len(valid_contours))\n",
    "        #img = cv2.drawContours(img.copy(), valid_contours, -1, (255,0,0), 10)\n",
    "\n",
    "    return img\n",
    "    #print(contours)\n",
    "        #cv2.imwrite(out_path + f'{image}_contouered_mask_{num + 1}.jpg', cv2.drawContours(cv2.cvtColor(mask * 255, cv2.COLOR_GRAY2RGB), contours, -1, (255,0,0), 10))\n",
    "\n",
    "#plt.imsave(out_path + 'image.jpg', mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884e7d1-f487-49e4-9c95-e0a7d3c822cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "False | False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82643db0-6b45-4d57-a6f3-617baee07dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contours[0].squeeze(1).shape\n",
    "cv2.imwrite(out_path + f'contouered_mask.jpg', cv2.drawContours(mask * 256, contours, -1, (255,0,0), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7f733-ea2f-4466-b8fc-f1a1be681a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'C:/Users/User/Petr/Net_4/test_model/images/'\n",
    "\n",
    "for image in os.listdir(test_path):\n",
    "    image_original = cv2.imread(test_path + image)\n",
    "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image_1 = image_original[2500:, 2500:]\n",
    "    cv2.imwrite(test_path + f'{image[:-4]}_1.jpg', cv2.cvtColor(image_1, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    image_2 = image_original[2500:, : 2500]\n",
    "    cv2.imwrite(test_path + f'{image[:-4]}_2.jpg', cv2.cvtColor(image_2, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    image_3 = image_original[:2500, 2500:]\n",
    "    cv2.imwrite(test_path + f'{image[:-4]}_3.jpg', cv2.cvtColor(image_3, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    image_4 = image_original[:2500, :2500]\n",
    "    cv2.imwrite(test_path + f'{image[:-4]}_4.jpg', cv2.cvtColor(image_4, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce6f13-1ce3-44c6-b6c1-802e8f83e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mask_model()\n",
    "model.eval()\n",
    "pred = model([image.to(device)])\n",
    "output = pred[0]\n",
    "\n",
    "visualize_masks(image.detach().cpu(), output, inv_classes, obj_colors=obj_colors, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae342514-e27d-499d-80c9-bac7f9c45418",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = get_mask_model_v2()\n",
    "model_v2.eval()\n",
    "pred_v2 = model_v2([image.to(device)])\n",
    "output_v2 = pred_v2[0]\n",
    "\n",
    "visualize_masks(image.detach().cpu(), output_v2, inv_classes, obj_colors=obj_colors, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a722067-1918-47c6-af8f-d8b2126abb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = 'C:/Users/User/Petr/Net_4/Dataset/masks/'\n",
    "files = os.listdir(mask_path)\n",
    "for mask in files:\n",
    "    if 'Asphalt road' in mask:\n",
    "        try:\n",
    "            new_name = mask.replace('Asphalt road', 'Road')\n",
    "            os.rename(mask_path + mask, mask_path + new_name)\n",
    "        except FileExistsError:\n",
    "            #new_name = mask.replace('Asphalt road', 'Road')\n",
    "            number = int(new_name.split('-')[-1][:-4])\n",
    "            new_name = '-'.join(new_name.split('-')[:-1]) + '-' + str(number + 1) + '.npy'\n",
    "            while new_name in os.listdir(mask_path):\n",
    "                number += 1\n",
    "                new_name = '-'.join(new_name.split('-')[:-1]) + '-' + str(number + 1) + '.npy'\n",
    "            os.rename(mask_path + mask, mask_path + new_name)\n",
    "    if 'Country road' in mask:\n",
    "        try:\n",
    "            new_name = mask.replace('Country road', 'Road')\n",
    "            os.rename(mask_path + mask, mask_path + new_name)\n",
    "        except FileExistsError:\n",
    "            #new_name = mask.replace('Country road', 'Road')\n",
    "            number = int(new_name.split('-')[-1][:-4])\n",
    "            new_name = '-'.join(new_name.split('-')[:-1]) + '-' + str(number + 1) + '.npy'\n",
    "            while new_name in os.listdir(mask_path):\n",
    "                number += 1\n",
    "                new_name = '-'.join(new_name.split('-')[:-1]) + '-' + str(number + 1) + '.npy'\n",
    "            os.rename(mask_path + mask, mask_path + new_name)\n",
    "    if 'Water' in mask:\n",
    "        os.remove(mask_path + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42afe3c-1d8e-4be6-9561-dc3debee07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask in os.listdir(mask_path):\n",
    "    if 'Country road' in mask:\n",
    "        os.remove(mask_path + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf1e82-e197-45df-8c0d-26ad65103f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "'task-3848-annotation-2700-by-1-tag-Country road-0.npy'.replace('Country road', 'Road')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60a4e3-483f-40ef-8a63-9ee11eb584ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'task-3850-annotation-2702-by-1-tag-Asphalt road-10.npy'\n",
    "\n",
    "number = int(file.split('-')[-1][:-4])\n",
    "\n",
    "print(number)\n",
    "\n",
    "new_file = '-'.join(file.split('-')[:-1]) + '-' + str(number + 1) + '.npy'\n",
    "\n",
    "print(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca84441-2637-4ef0-9110-cefb9cae6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "'-'.join(file.split('-')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79c024-370e-4002-9a35-4653ac77e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = pd.read_json('C:/Users/User/Petr/Net_4/Dataset/annotations.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447dd49-f7d3-4e8c-9e29-82f9c6bd3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989ee61-e290-4548-87e9-6564aa64f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann[ann['tag'][0][0]['brushlabels'] != ['Water']].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c1ad6-6c02-43b8-bb54-8406875c9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for data in ann['tag'][0] for label in data['brushlabels']]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c0e10-7d03-4dfe-b5ee-bffbdd94e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "indxs = []\n",
    "for row in ann.iterrows():\n",
    "    labels = []\n",
    "    for data in row[1].tag:\n",
    "        labels.extend(data['brushlabels'])\n",
    "    if 'Water' not in labels:\n",
    "        indxs.append(row[0])\n",
    "\n",
    "print(indxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a643e5d-c102-4472-bea0-3ad506fdb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_1 = ann.iloc[indxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a269a-6fe5-4402-b7c7-90912d691341",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e23c08-b30a-4380-8e9f-f01a04b73ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_1.iloc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541cdfd-7387-46e5-8b06-509e808a248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['task-3848-annotation-2700-by-1-tag-Asphalt road-0.npy', 'task-3848-annotation-2700-by-1-tag-Country road-0.npy']\n",
    "\n",
    "masks = [np.load(path + 'masks/' + mask_path)[2500:, :2500] for mask_path in paths]\n",
    "\n",
    "#cut_mask = mask[2500:, 2500:]\n",
    "\n",
    "masks = torch.as_tensor(np.array(masks), dtype=torch.bool)\n",
    "\n",
    "print(type(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d8779-0d82-4f22-805a-61bcbf2926b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(path + 'images/mmih-0-0.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = image[2500:, :2500]\n",
    "cv2.imwrite(path + 'crop.jpg', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "image = F.to_tensor(image)\n",
    "image = (image * 255).to(dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d80ce-1623-472d-9408-85834cde2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "masked_image = draw_segmentation_masks(image=image, masks=masks)\n",
    "print(type(masked_image))\n",
    "masked_image = F.to_pil_image(masked_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5e181-53a9-4ab7-bc6f-971a40dd4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(np.asarray(masked_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c11a9-3fb3-4dbf-89ff-36c83291d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_json(path + 'annotations.json', orient='records')\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61eea9-cd54-4be5-8fdf-757dc64bf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_paths = annotations['masks']\n",
    "\n",
    "areas = []\n",
    "\n",
    "for paths in tqdm(mask_paths):\n",
    "    masks = np.array([np.load(path) for path in paths])\n",
    "    bbox = masks_to_boxes(torch.tensor(masks, dtype=torch.uint8)).detach().cpu().numpy()\n",
    "    areas.extend(((bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0])))\n",
    "\n",
    "print(areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f0817-fd13-4e78-a196-56ef8b810f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [area / 6250000 for area in areas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d3583-d6c1-4651-a7d8-b2aaad0cf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "sns.boxplot(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68e60d-3c68-4a97-88be-b4060df97ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.load(\"C:/Users/User/Petr/Net_4/Dataset/masks/task-3976-annotation-2832-by-1-tag-Asphalt road-0.npy\")\n",
    "\n",
    "print(type(mask), mask.shape)\n",
    "\n",
    "mask_1 = torch.from_numpy(mask)\n",
    "\n",
    "print(mask_1, type(mask_1), mask_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25572a3d-4add-40d9-9c69-fb63e1867a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.load(mask_paths[0])\n",
    "\n",
    "bbox = masks_to_boxes(torch.tensor([mask], dtype=torch.uint8)).detach().cpu().numpy()\n",
    "area = ((bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0]))[0]\n",
    "print(area, 2500*2500, area / (2500*2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0290e9-c8c4-46e1-b2ad-31da0a38c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "new_path = 'C:/Users/User/Petr/Net_4/Dataset_4_single/'\n",
    "\n",
    "rows = annotations.iterrows()\n",
    "\n",
    "total_area = 6250000\n",
    "\n",
    "print(\"Start converting\")\n",
    "\n",
    "for num, row in enumerate(rows):\n",
    "    task_1, task_2, task_3, task_4 = {}, {}, {}, {}\n",
    "    id = row[1]['id']\n",
    "    image_path = path + 'images/' + '-'.join(row[1]['image'].split('-')[1:])\n",
    "    image_name = '-'.join(row[1]['image'].split('-')[1:])[:-4]\n",
    "    image_original = cv2.imread(image_path)\n",
    "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
    "    masks = [(np.load(path + 'masks/' + mask), mask) for mask in os.listdir(path + 'masks/') if f'task-{id}' in mask]\n",
    "\n",
    "    image = image_original[2500:, 2500:]\n",
    "    cv2.imwrite(new_path + f'images/{image_name}_1.jpg', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    task_1['image_path'] = new_path + f'images/{image_name}_1.jpg'\n",
    "\n",
    "    image = image_original[2500:, :2500]\n",
    "    cv2.imwrite(new_path + f'images/{image_name}_2.jpg', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    task_2['image_path'] = new_path + f'images/{image_name}_2.jpg'\n",
    "    \n",
    "    image = image_original[:2500, 2500:]\n",
    "    cv2.imwrite(new_path + f'images/{image_name}_3.jpg', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    task_3['image_path'] = new_path + f'images/{image_name}_3.jpg'\n",
    "    \n",
    "    image = image_original[:2500, :2500]\n",
    "    cv2.imwrite(new_path + f'images/{image_name}_4.jpg', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    task_4['image_path'] = new_path + f'images/{image_name}_4.jpg'\n",
    "    \n",
    "    #masks = [np.where(mask[:,...] > 0, 1) for mask in masks]\n",
    "\n",
    "    task_1['masks'] = []\n",
    "    task_2['masks'] = []\n",
    "    task_3['masks'] = []\n",
    "    task_4['masks'] = []\n",
    "\n",
    "    task_1['id'] = id\n",
    "    task_2['id'] = id\n",
    "    task_3['id'] = id\n",
    "    task_4['id'] = id\n",
    "\n",
    "    for num_m, mask in enumerate(masks):\n",
    "        mask_type = '-'.join(mask[1].split('-')[-2:])[:-4]\n",
    "        if 'Water' in mask_type:\n",
    "            continue\n",
    "        if mask[0].shape[0] != 5000 or mask[0].shape[1] != 5000:\n",
    "            continue\n",
    "        print(mask_type)\n",
    "\n",
    "        mask_np = mask[0][2500:, 2500:]\n",
    "        if np.any(mask_np):\n",
    "            bbox = masks_to_boxes(torch.tensor(mask_np.reshape(1, 2500, 2500), dtype=torch.uint8)).detach().cpu().numpy()\n",
    "            area = ((bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0]))[0]\n",
    "            if area / total_area > 0.01:\n",
    "                np.save(new_path + f'masks/{image_name}_1-Road-{num_m}.npy', mask_np)\n",
    "                task_1['masks'].append(new_path + f'masks/{image_name}_1-Road-{num_m}.npy')\n",
    "\n",
    "        mask_np = mask[0][2500:, :2500]\n",
    "        if np.any(mask_np):\n",
    "            bbox = masks_to_boxes(torch.tensor(mask_np.reshape(1, 2500, 2500), dtype=torch.uint8)).detach().cpu().numpy()\n",
    "            area = ((bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0]))[0]\n",
    "            if area / total_area > 0.01:\n",
    "                np.save(new_path + f'masks/{image_name}_2-Road-{num_m}.npy', mask_np)\n",
    "                task_2['masks'].append(new_path + f'masks/{image_name}_2-Road-{num_m}.npy')\n",
    "        \n",
    "        mask_np = mask[0][:2500, 2500:]\n",
    "        if np.any(mask_np):\n",
    "            bbox = masks_to_boxes(torch.tensor(mask_np.reshape(1, 2500, 2500), dtype=torch.uint8)).detach().cpu().numpy()\n",
    "            area = ((bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0]))[0]\n",
    "            if area / total_area > 0.01:\n",
    "                np.save(new_path + f'masks/{image_name}_3-Road-{num_m}.npy', mask_np)\n",
    "                task_3['masks'].append(new_path + f'masks/{image_name}_3-Road-{num_m}.npy')\n",
    "        \n",
    "        mask_np = mask[0][:2500, :2500]\n",
    "        if np.any(mask_np):\n",
    "            bbox = masks_to_boxes(torch.tensor(mask_np.reshape(1, 2500, 2500), dtype=torch.uint8)).detach().cpu().numpy()\n",
    "            area = ((bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0]))[0]\n",
    "            if area / total_area > 0.01:\n",
    "                np.save(new_path + f'masks/{image_name}_4-Road-{num_m}.npy', mask_np)\n",
    "                task_4['masks'].append(new_path + f'masks/{image_name}_4-Road-{num_m}.npy')\n",
    "\n",
    "    if len(task_1['masks']) > 0:\n",
    "        data.append(task_1)\n",
    "    if len(task_2['masks']) > 0:\n",
    "        data.append(task_2)\n",
    "    if len(task_3['masks']) > 0:\n",
    "        data.append(task_3)\n",
    "    if len(task_4['masks']) > 0:\n",
    "        data.append(task_4)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f'{num}/{len(annotations)} compleate...')\n",
    "\n",
    "\n",
    "df_ann = pd.DataFrame(data=data)\n",
    "df_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbf541-03f9-4d54-821c-c5e56b9078c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.iloc[86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91899cc1-2c66-4f13-beb5-09393a33ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ann.to_json(new_path + 'annotations.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824eb933-3aec-4993-b6d2-bd1fac2f78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ann.loc[df_ann['id'] == 3975]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91307b-d4da-4adc-ae98-b453557011e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = annotations.drop(annotations['masks'] != [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720b149-f72e-494f-9634-0b4e9971b620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
